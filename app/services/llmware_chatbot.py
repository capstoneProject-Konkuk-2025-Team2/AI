# llmware RAG ì±—ë´‡ 
# ========== 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒì„± ë° csv íŒŒì¼ íŒŒì‹± ==========
from llmware.library import Library
from llmware.retrieval import Query
from llmware.models import ModelCatalog
from llmware.setup import Setup
from llmware.configs import LLMWareConfig, MilvusConfig
from llmware.prompts import Prompt
from app.services.segmantic_rag import semantic_rag
import os

def prepare_library(library_name="my_library", document_folder="app/data/my_csv_folder"):
    """ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒì„± + í´ë” ì•ˆ csv íŒŒì¼ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜ """
    # DB ì„¤ì •
    LLMWareConfig().set_active_db("sqlite")
    
    ingestion_folder_path = document_folder
    
    # ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒì„±
    library = Library().create_new_library(library_name)
    print(f" ë¼ì´ë¸ŒëŸ¬ë¦¬ '{library_name}' ìƒì„± ì™„ë£Œ.")

    # í´ë” ì•ˆ íŒŒì¼ë“¤ ì¶”ê°€
    parsing_output = library.add_files(ingestion_folder_path)
    print(f" CSV íŒŒì‹± ì™„ë£Œ: {parsing_output}")

    return library

# ========== 2. ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ì„ë² ë”© ì ìš© ==========
def install_embeddings(library, embedding_model="mini-lm-sbert"):
    """ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ì„ë² ë”© ì„¤ì¹˜í•˜ëŠ” í•¨ìˆ˜ """
    # ë²¡í„° DB ì„¤ì •
    MilvusConfig().set_config("lite", True)
    LLMWareConfig().set_vector_db("chromadb")

    print(f" ë²¡í„° ì„ë² ë”© ì‹œì‘ - ëª¨ë¸: {embedding_model}")
    library.install_new_embedding(embedding_model_name=embedding_model, vector_db="chromadb", batch_size=100)
    print(" ë²¡í„° ì„ë² ë”© ì™„ë£Œ.")

# ========== 3. ì±—ë´‡ ë©”ì¸ ë£¨í”„ ==========
def start_chatbot(library_name, model_name="bling-large"):
    """ RAG ì±—ë´‡ ì‹¤í–‰ í•¨ìˆ˜ """

    # ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ëª¨ë¸ ë¡œë“œ
    lib = Library().load_library(library_name)
    query_engine = Query(lib)
    model = ModelCatalog().load_model(model_name)

    print("\n RAG ì±—ë´‡ì— ì˜¤ì‹  ê±¸ í™˜ì˜í•©ë‹ˆë‹¤!")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥\n")

    while True:
        user_question = input(" ì§ˆë¬¸: ")

        if user_question.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
            print("ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # semantic queryë¥¼ í†µí•´ ìœ ì‚¬ ë¬¸ì¥ ê²€ìƒ‰
        search_results = query_engine.semantic_query(user_question, result_count=3)

        # ğŸ” ë¦¬íŠ¸ë¦¬ë²„ ê²°ê³¼ ë¡œê·¸ ì¶œë ¥
        print("\n[ğŸ” ìœ ì‚¬ ë¬¸ì¥ ê²€ìƒ‰ ê²°ê³¼]")
        for idx, res in enumerate(search_results):
            print(f"{idx+1}. {res['text'][:200]}...")  # ê¸¸ì´ ìë¥´ê¸°

        if not search_results:
            print(" ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n")
            continue

        # ê²€ìƒ‰ëœ ìƒìœ„ 3ê°œ ë¬¸ì¥ì„ ë¬¸ë§¥ìœ¼ë¡œ í•©ì¹˜ê¸°
        # combined_context = " ".join([res["text"] for res in search_results])
        # ìˆ˜ì •: ê¸¸ì´ ì œí•œ ì¶”ê°€
        max_context_tokens = 512  # ì˜ˆì‹œ: 512 í† í°ìœ¼ë¡œ ì œí•œ
        combined_context = " ".join([res["text"] for res in search_results])[:max_context_tokens]

        # ê²€ìƒ‰ëœ ë¬¸ì„œì—ì„œ ë¬¸ë§¥ êµ¬ì„±
        filtered_contexts = [res["text"] for res in search_results if len(res["text"]) > 20]
        combined_context = " ".join(filtered_contexts)[:max_context_tokens]

        # ëª…ì‹œì  prompt êµ¬ì„±
        final_prompt = f"""ë‹¹ì‹ ì€ ë¹„êµê³¼ í”„ë¡œê·¸ë¨ì„ ì¶”ì²œí•´ì£¼ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.
        ì§ˆë¬¸: "{user_question}"
        ê´€ë ¨ ì •ë³´:
        {combined_context}
        ë‹µë³€:"""

        # ëª¨ë¸ ì‘ë‹µ ìƒì„±
        response = model.inference(final_prompt)

        # ê²°ê³¼ ì¶œë ¥
        print("\n ë‹µë³€:", response["llm_response"], "\n")

        # # ëª¨ë¸ë¡œ ì§ˆë¬¸ + ë¬¸ë§¥ ê¸°ë°˜ ë‹µë³€ ìƒì„±
        # response = model.inference(user_question, add_context=combined_context)

        # # print("\n ë‹µë³€:", response, "\n")
        # print("\n ë‹µë³€:", response["llm_response"], "\n")

# ========== 4. ì „ì²´ ì‹¤í–‰ ==========
if __name__ == "__main__":
    library_name = "my_library"  # ì›í•˜ëŠ” ì´ë¦„
    document_folder =  "app/data/my_csv_folder"  # í´ë” ì´ë¦„ìœ¼ë¡œ ìˆ˜ì •
    embedding_model = "mini-lm-sbert"  # ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸
    llm_model_name = "bling-answer-tool"  # ì‚¬ìš©í•  LLM ëª¨ë¸

    # ê¸°ì¡´ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚­ì œ (ì¤‘ë³µ ë°©ì§€ìš©)
    try:
        Library().delete_library(library_name)
        print(f" ë¼ì´ë¸ŒëŸ¬ë¦¬ '{library_name}' ì‚­ì œ ì™„ë£Œ.")
    except:
        print(f" ë¼ì´ë¸ŒëŸ¬ë¦¬ '{library_name}' ì‚­ì œ ì‹¤íŒ¨ (ì—†ì—ˆê±°ë‚˜ ì´ë¯¸ ì‚­ì œë¨).")

    # 1ë‹¨ê³„: ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒì„± ë° ë¬¸ì„œ íŒŒì‹±
    library = prepare_library(library_name, document_folder)

    # 2ë‹¨ê³„: ë²¡í„° ì„ë² ë”© ìƒì„±
    install_embeddings(library, embedding_model)

    # 3ë‹¨ê³„: ì±—ë´‡ ì‹¤í–‰ (ë‘˜ ì¤‘ ì„ íƒ)
    mode = input("ëª¨ë“œ ì„ íƒ (1: ê¸°ë³¸ ì±—ë´‡ / 2: Semantic Query ê°•í™” ì±—ë´‡): ")

    if mode == "1":
        start_chatbot(library_name, llm_model_name)
    elif mode == "2":
        semantic_rag(library_name, llm_model_name)
    else:
        print("ì˜¬ë°”ë¥¸ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”.")
